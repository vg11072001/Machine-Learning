# Research Odyssey: Papers, Insights, and Architectures

This directory contains various research papers and notes related to machine learning, deep learning, Trannsformer and any about tools to better understanding. Below is an overview of the contents:

### Time Series Forecasting and Language Models
1. **Context is Key: A Benchmark for Forecasting with Essential Textual Information**  
   - **Focus**: Combining textual information with time-series forecasting.  

2. **Tiny Time Mixers: Fast Pre-Trained Models for Enhanced Zero- and Few-Shot Forecasting of Multivariate Time Series**  
   - **Focus**: Lightweight models for time-series forecasting with minimal data.  

3. **Chronos: Learning the Language of Time Series**  
   - **Publisher**: Amazon.  
   - **Focus**: Advances in understanding time-series data as a language.  

4. **Time-Series Forecasting with Decoder-Only Foundation Models**  
   - **Publisher**: Google.  
   - **Focus**: Decoder-only architectures for time-series modeling.  

5. **Time-Series-LLM.pdf**  
   - **Focus**: Papers on applying large language models (LLMs) to time-series data.  

---

### Vision Transformers (ViT)
1. **Vision Transformer (ViT): From Scratch**  
   - **Focus**: Organizing and implementing the original Vision Transformer architecture.  

### **BLT: Patches Scale Better than Tokens**  
   - **Focus**: Exploring how patches outperform tokens in ViT architectures.  

---

- **Extra.md**  
   - **Summary**: Contains a collection of additional insights on ViT and time-series LLM papers.  

- **Papers Readme.md**  
   - **Summary**: A consolidated overview of ViT and time-series LLM papers.  

---

## Repository Updates
- **Create Readme.md**: 2 minutes ago.
- **Extra.md**: Papers of ViT and time-series LLM added.  
- **Time-series-forecasting-decoder-only-foundation-model-by-google.pdf**: Added 2 weeks ago.  
- **Tiny Time Mixers**: Added 3 weeks ago.  

---

### Note
This repository is an evolving workspace as I explore new research areas. Contributions and suggestions are welcome!
