%%

# Coding Sequence

- [ ] start with neural networks
- [ ] go for deep neural network

Inspired from GPT Learning Hub:
- Generative LLMs: Basic ML to Implementation of Attention All you need and training own mini LLM
	- start with neural networks/deep neural networks
		- [x] NN
		- [ ] CNN 
			- [ ] Image processing [github](https://github.com/BhanuPrakashNani/Image_Processing?tab=readme-ov-file) [Tutorial](https://github.com/askitlouder/Image-Processing-Tutorials)
		- [ ] Some complex mode
	- go for already implemented PyTorch papers/CNN or NN based
		- [ ] VIT - CNN + Transformer
		- [ ] Car project decode it
		- [ ] idea from GTP Hub YouTube
		- [ ] Kaggle already implemented comp. winning code-> decode.
		- [ ] training your own mini LLM [link](https://www.google.com/search?sca_esv=a88b1582b82b9395&rlz=1C1UEAD_en-GBIN1073IN1073&biw=1920&bih=869&sxsrf=ADLYWILY5Ox0diBU79ZdAMV8bmIN8LO2pA:1731785005592&q=training+your+own+mini+LLM&tbm=vid&source=lnms&fbs=AEQNm0CvspUPonaF8UH5s_LBD3JPX4RSeMPt9v8oIaeGMh2T2PRrsfVPlQRxSTpQ4UUI6wfsFlEVaMALnJjEZtYpSTLmUV5oGF4fnHSG0LbvLjVKUV0IWX-9yHknaXpsINbxRPK_rD0aGBXyqo-cUa2T6ZySNg4d875n-vXkSutq7bWvEyjXRQuBfWktFSUkoyoeiqhKHNE-BZiSEnsk93CG_Dl8i5jDTw&sa=X&ved=2ahUKEwiwutfwyeGJAxXhUWwGHR4PNSwQ0pQJegQIERAB)
		- [ ] implementation of attention all you need [link](https://www.google.com/search?sca_esv=a88b1582b82b9395&rlz=1C1UEAD_en-GBIN1073IN1073&sxsrf=ADLYWILDKywN5vMXyR9_XxXcJJIjI3cwKg:1731784949211&q=implementing+Attention+Is+All+You+Need&tbm=vid&source=lnms&fbs=AEQNm0CvJOjjOlYGHpeb6_mX0N9Pz_0vXmyFV6JRst0G88bAGyAKbn3N-PJtAQ9FF1FHZrb3Vxmn8XVEU7WdIN3ev85T-c9yjSuUQM7DyejoPmLMLBxzIHnbdRCYBmryCtnMt_A6w4k9cULSoBpLrQj8FRnSr6R4dg1dQPiX7wbTGa4jNRr-1EsMaELCwmIPmbwPfYZhNgPaqfijDAjpxqO9DWXY4bKmNw&sa=X&ved=2ahUKEwjunebVyeGJAxU_UGcHHaWABCMQ0pQJegQIEhAB&biw=1920&bih=869&dpr=1)
		- [ ] class and object in python tutorial for ml model design

- Fine tunning LLMs using open source like Llama from Meta [link](https://www.google.com/search?q=fine+tune+open+source+models+like+Llama+from+Meta+(without+an+expensive+GPU).&rlz=1C1UEAD_en-GBIN1073IN1073&sourceid=chrome&ie=UTF-8)
	- [ ] take some hugging face models

- to build Multimodal AI - PyTorch and transformer [link](https://www.google.com/search?q=how+to+build+multimodal+AI+pytorch+transformers&sca_esv=a88b1582b82b9395&rlz=1C1UEAD_en-GBIN1073IN1073&sxsrf=ADLYWIK2BnE1C7csUJqtna1ks5vQGcp-Jg%3A1731785123846&ei=o_E4Z-uvM_qhnesPxo_DmAQ&ved=0ahUKEwirjYmpyuGJAxX6UGcHHcbHEEMQ4dUDCA8&uact=5&oq=how+to+build+multimodal+AI+pytorch+transformers&gs_lp=Egxnd3Mtd2l6LXNlcnAiL2hvdyB0byBidWlsZCBtdWx0aW1vZGFsIEFJIHB5dG9yY2ggdHJhbnNmb3JtZXJzMgUQIRigATIFECEYoAFI3IkBULIFWJ-GAXADeAKQAQCYAb0CoAHuJ6oBCDAuMTUuNy4yuAEDyAEA-AEBmAIcoALfKMICBBAAGEfCAgYQABgWGB7CAgsQABiABBiGAxiKBcICCBAAGIAEGKIEwgIHECEYoAEYCsICBBAhGBXCAgUQIRifBZgDAIgGAZAGCJIHCDQuMTMuOS4yoAe1Ug&sclient=gws-wiz-serp)
- amazon competition can add on kaggle [Video link](https://www.twitch.tv/videos/1804684510) /  [Github of ranked solution](https://github.com/greenfish8090/AmazonML?tab=readme-ov-file) / Amazon [Site](https://www.hackerearth.com/challenges/competitive/amazon-ml-challenge-2023/leaderboard/)
Extra: 
- 50-AI/ML papers + technical breakdown of content
- optimize resume
%%

# Resources

### Basic ML from scratch
https://github.com/rushter/MLAlgorithms
https://github.com/fchollet/deep-learning-with-python-notebooks
https://github.com/eriklindernoren/ML-From-Scratch/
https://github.com/patrickloeber/MLfromscratch
https://github.com/alirezadir/Machine-Learning-Interviews/blob/main/src/ml-fundamental.md
https://github.com/mrdbourke/zero-to-mastery-ml

Also traditional algorithms available: https://mlcourse.ai/book/prereqs/python.html

- Neural Networks
	- Theory/NN ZerotoHero Series/nn-zero-to-hero/[[Theory/NN ZerotoHero Series/nn-zero-to-hero/README|README]]
	- https://github.com/souvikmajumder26/Neural-Network-from-Scratch
	- Simple MNIST NN from scratch only NumPy, no TF/Keras [link](https://www.kaggle.com/code/wwsalmon/simple-mnist-nn-from-scratch-numpy-no-tf-keras)
### PyTorch:

- Notebook attached on folder
- [[Theory/ML with PyTorch and Sklearn/ch12/README]]
- https://pytorch.org/docs/stable/index.html

github.com/eriklindernoren/PyTorch-GAN
https://github.com/ritchieng/the-incredible-pytorch
https://www.learnpytorch.io/00_pytorch_fundamentals/#introduction-to-tensors
https://github.com/mrdbourke/pytorch-deep-learning

### TensorFlow:
- Official doc https://www.tensorflow.org/guide/core/quickstart_core
- Notebook [link](https://github.com/vg11072001/Machine-Learning/blob/4fdcffddc18a88f6f9e4601ab914cb55bba4f06b/ml%20notes/MLCoding/tensorflowtrain.ipynb)

https://github.com/tensorflow/docs
https://github.com/mrdbourke/tensorflow-deep-learning
https://github.com/aymericdamien/TensorFlow-Examples
https://github.com/BaoLocPham/TensorFlow-Advanced-Techniques-Specialization
https://github.com/tensorflow/models/tree/master

https://github.com/BhanuPrakashNani/Image_Processing?tab=readme-ov-file

### Deep learning / CV models:
- Deep neural network specialization
- deep learning +cv course of Sandford
- course : https://www.eecs.yorku.ca/~kosta/Courses/EECS4422/ Computer vision
- course : https://www.eecs.yorku.ca/~kosta/Courses/EECS6322/ Deep learning in CV

- PyTorch Tutorial 14 - Convolutional Neural Network (CNN): https://www.youtube.com/watch?v=pDdP0TFzsoQ
 - NumPyCNN: Implementing Convolutional Neural Networks From Scratch: https://github.com/ahmedfgad/NumPyCNN
 - Convolutional Neural Network from Scratch | Mathematics & Python Code: https://www.youtube.com/watch?v=Lakz2MoHy6o
 
https://www.deeplearningbook.org/
https://github.com/AlessandroSaviolo/CNN-from-Scratch
https://github.com/vzhou842/cnn-from-scratch

### GPT/LLM/Transformer:
- [GPT-Transformer](GPT-Transformer.md)
- GitHub my on NLP

## Extra

Machine Learning: From Basics to Advanced Techniques.  

ðŸŽ¯ Linear Regression: [lnkd.in/gdRsMHbn](http://lnkd.in/gdRsMHbn)  
ðŸŽ¯ Logistic Regression: [lnkd.in/gtPfmQUv](http://lnkd.in/gtPfmQUv)  
ðŸŽ¯ Data Science basics, Variable types, ML & Stats: [youtu.be/maxyUZGB3QY](http://youtu.be/maxyUZGB3QY), [youtu.be/Y6PEpkEdXDQ](http://youtu.be/Y6PEpkEdXDQ)  
ðŸŽ¯ Isotonic Regression: [youtu.be/lo3rUyk9qi0](http://youtu.be/lo3rUyk9qi0)  
ðŸŽ¯ ML metrics for Classification: [youtu.be/E2HRSJKU-_4](http://youtu.be/E2HRSJKU-_4)  
ðŸŽ¯ Naive Bayes Classifier: [youtu.be/IvTCdrx1SHQ](http://youtu.be/IvTCdrx1SHQ)  
ðŸŽ¯ Dimensionality Reduction: PCA, AutoEncoders: [https://lnkd.in/gC6XQfez](https://lnkd.in/gC6XQfez)  
ðŸŽ¯ Introduction to Entropy, Cross-Entropy, KL-Divergence: [https://lnkd.in/gSXh8ZX8](https://lnkd.in/gSXh8ZX8)  
ðŸŽ¯ Probability, Model Calibration: [youtu.be/rG2EfFOXyg0](http://youtu.be/rG2EfFOXyg0)  
ðŸŽ¯ Data Drift Detection, Model Monitoring: [youtu.be/tQjRQWfYQ10](http://youtu.be/tQjRQWfYQ10)  
ðŸŽ¯ Dynamic pricing in Ecommerce: [youtu.be/a_CXpnsvPa0](http://youtu.be/a_CXpnsvPa0)  
ðŸŽ¯ Training Embeddings for Recommendation Systems: [youtu.be/DN4S96oHRhE](http://youtu.be/DN4S96oHRhE)  
ðŸŽ¯ Annoy for calculating ANN in Recsys: [youtu.be/DSQOrBTqmYA](http://youtu.be/DSQOrBTqmYA)  
ðŸŽ¯ Product Quantizer for calculating ANN in Recsys: [youtu.be/50PNumB7s3U](http://youtu.be/50PNumB7s3U)  
ðŸŽ¯ Model-Based Account Recommendations @ Twitter: [youtu.be/Xqo8fwgjxW4](http://youtu.be/Xqo8fwgjxW4)  
ðŸŽ¯ PID Controller for diversity in recommender systems: [youtu.be/laTxgnzjfR0](http://youtu.be/laTxgnzjfR0)  
ðŸŽ¯ Lessons from Instagramâ€™s Recommendation System: [youtu.be/Myna6rnmCG8](http://youtu.be/Myna6rnmCG8)  
ðŸŽ¯ Train Neural Networks to Approximate any Function: [youtu.be/4PvGKuqRQTE](http://youtu.be/4PvGKuqRQTE)  
ðŸŽ¯ How LLM Bert works, pre-trained Bert for Embeddings: [youtu.be/v-0J7o-nDBE](http://youtu.be/v-0J7o-nDBE)  
ðŸŽ¯ Twitter's Recommendation Algorithm: [youtu.be/IhGq9jgcxFM](http://youtu.be/IhGq9jgcxFM)  
ðŸŽ¯ Model Compression with Knowledge Distillation: [youtu.be/1N_EBJUOjVU](http://youtu.be/1N_EBJUOjVU)  
ðŸŽ¯ Conversational AI: Tech behind Chat-GPT: [youtu.be/JKoJ5YIr2O4](http://youtu.be/JKoJ5YIr2O4)  
ðŸŽ¯ Dual Nature of Conversational LLMs: [youtu.be/MHfzoHC4kek](http://youtu.be/MHfzoHC4kek)  
ðŸŽ¯ Pushing Boundaries of LLMs: Sparse & Flash Attention, Quantisation, Pruning, Distillation, LORA: [youtu.be/mF7OM_XU2S4](http://youtu.be/mF7OM_XU2S4)  
ðŸŽ¯ Falcon & LLAMA-2 Open-Source LLM: Custom Fine-tuning on a Single-GPU Colab | PEFT | LORA: [youtu.be/CxqZ5j3xlt0](http://youtu.be/CxqZ5j3xlt0), [youtu.be/8cc4bJtycOA](http://youtu.be/8cc4bJtycOA)  
ðŸŽ¯ Supercharging LLama-2 & Falcon: Enhancing Performance on Any Task with ChatGPT Dataset: [youtu.be/paGr-t1wSOQ](http://youtu.be/paGr-t1wSOQ), [youtu.be/lo11Iczb0Vc](http://youtu.be/lo11Iczb0Vc)  
ðŸŽ¯ The Making of SRKGPT: Crafting an AI with Shahrukh Khan's Style LLama v2 Finetuning [youtu.be/gYPwx0DR7zc](http://youtu.be/gYPwx0DR7zc)  
ðŸŽ¯ Innovative Linkedin's Deep-leaning CTR Modeling: Deep, Wide, and Shallow Towers Explained: [youtu.be/7l0HLYVFEuU](http://youtu.be/7l0HLYVFEuU)  
ðŸŽ¯ Meituan's Game-Changing Two-Tower Retrieval Recsys Model: [youtu.be/UhpbTSbi3lI](http://youtu.be/UhpbTSbi3lI)  
ðŸŽ¯ IIIT Sricity: Navigating Twitter & Instagram Recommender Systems: [youtu.be/PaDsiJCPCXQ](http://youtu.be/PaDsiJCPCXQ)  
ðŸŽ¯ Building Scalable Query-Item Two-Tower Model based Retrieval System: [youtu.be/o-pZk5R0TZg](http://youtu.be/o-pZk5R0TZg)  
ðŸŽ¯ Overcoming Biases for a Recsys: [youtu.be/oGb_mIdO0tA](http://youtu.be/oGb_mIdO0tA)  
ðŸŽ¯ Evolution of Recsys: [youtu.be/lgoyJn7MsH8](http://youtu.be/lgoyJn7MsH8)  
ðŸŽ¯ Multi-Armed Bandit Startegies: [youtu.be/2A5f3GrX0dA](http://youtu.be/2A5f3GrX0dA)  
ðŸŽ¯ Netflix's Unified Recommendation ML Model: [youtu.be/OKmv9sUrvk8](http://youtu.be/OKmv9sUrvk8)  
ðŸŽ¯ Netflix's Calibrated Recommendations [youtu.be/DOWXNrBpO4w](http://youtu.be/DOWXNrBpO4w)

# Links:
- [[Deeplearning]]
- Notebooks:
