

## Distribution in pytorch

- Multi node training with PyTorch DDP, torch.distributed.launch, torchrun and mpirun [video link](https://www.youtube.com/@lambdacloud)
- Distribution pytorch guide: [dist_overview](https://pytorch.org/tutorials/beginner/dist_overview.html)
- Intro to **Invited Talk - PyTorch Distributed (DDP, RPC) - By Facebook Research Scientist Shen Li** : [link](https://www.youtube.com/watch?v=3XUG7cjte2U)
- https://www.youtube.com/@NVIDIADeveloper
	- ![](Pasted%20image%2020241218013405.png)
	- ![](Pasted%20image%2020241218013503.png)if u check for A 100 architecture the speeds will get double
	- ![](Pasted%20image%2020241218013627.png)
	- ![](Pasted%20image%2020241218013740.png)
	- ![](Pasted%20image%2020241218013831.png)
	- ![](Pasted%20image%2020241218013846.png)
	- ![](Pasted%20image%2020241218013914.png)
	- ![](Pasted%20image%2020241218013932.png)
	- ![](Pasted%20image%2020241218013953.png)
	- ![](Pasted%20image%2020241218014023.png)
	- ![](Pasted%20image%2020241218014046.png)
	- ![](Pasted%20image%2020241218014118.png)
	- ![](Pasted%20image%2020241218014127.png)
	- ![](Pasted%20image%2020241218014149.png)
	- ![](Pasted%20image%2020241218014200.png) mutiple gpu using hovord
	- https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/
	- ![](Pasted%20image%2020241218014321.png)
	- ![](Pasted%20image%2020241218014335.png)
	- ![](Pasted%20image%2020241218014403.png)
	- ![](Pasted%20image%2020241218015237.png)
	- ![](Pasted%20image%2020241218015257.png)
	- ![](Pasted%20image%2020241218015913.png)
	- ![](Pasted%20image%2020241218020051.png)
