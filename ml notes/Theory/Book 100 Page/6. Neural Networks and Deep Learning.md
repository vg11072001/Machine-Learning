## Neural Networks

### Multilayer Perceptron Example

### Feed-Forward Neural Network Architecture

## Deep Learning
* Vanishing gradient:
	* Usually we will be performing backpropagation using chain rule, 
	* when multiplied with n number of small values in layers becomes vanishes
	* to solves this some techniques like ReLu, LSTM and skip connection used in RNN also modern modification in gradient descent algorithm.
* Exploding gradient: dealt with
	* gradient clipping
	* l1 and l2 regularization

### Convolutional Neural Network
* 

#### Recommended Reading

- [Convolutional Neural Networks: Architectures, Convolution / Pooling Layers](http://cs231n.github.io/convolutional-networks/ "http://cs231n.github.io/convolutional-networks/") by Andrej Karpathy
- [Convolutional Neural Networks from the ground up](https://towardsdatascience.com/convolutional-neural-networks-from-the-ground-up-c67bb41454e1 "https://towardsdatascience.com/convolutional-neural-networks-from-the-ground-up-c67bb41454e1") by Alejandro Escontrela
- [Deep learning](http://neuralnetworksanddeeplearning.com/chap6.html "http://neuralnetworksanddeeplearning.com/chap6.html") by Michael Nielsen

### Recurrent Neural Network
* 


#### Recommended Reading
- [An extended version of Chapter 6 with RNN unfolding and bidirectional RNN](https://www.dropbox.com/s/ouj8ddydc77tewo/ExtendedChapter6.pdf?dl=0 "https://www.dropbox.com/s/ouj8ddydc77tewo/ExtendedChapter6.pdf?dl=0")
- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/ "http://karpathy.github.io/2015/05/21/rnn-effectiveness/") by Andrej Karpathy (2015)
- [Recurrent Neural Networks and LSTM](https://towardsdatascience.com/recurrent-neural-networks-and-lstm-4b601dd822a5 "https://towardsdatascience.com/recurrent-neural-networks-and-lstm-4b601dd822a5") by Niklas Donges (2018)
- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/ "http://colah.github.io/posts/2015-08-Understanding-LSTMs/") by Christopher Olah (2015)
- [Introduction to RNNs](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/ "http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/") by Denny Britz (2015)
- [Implementing a RNN with Python, Numpy and Theano](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/ "http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/") by Denny Britz (2015)
- [Backpropagation Through Time and Vanishing Gradients](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/ "http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/") by Denny Britz (2015)
- [Implementing a GRU/LSTM RNN with Python and Theano](http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/ "http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/") by Denny Britz (2015)
- [Simplified Minimal Gated Unit Variations for Recurrent Neural Networks](https://arxiv.org/abs/1701.03452 "https://arxiv.org/abs/1701.03452") by Joel Heck and Fathi Salem (2017)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762 "https://arxiv.org/abs/1706.03762") by Vaswani et al. (2017), a state-of-the-art sequence-to-sequence model, plus an [illustrated guide](http://jalammar.github.io/illustrated-transformer/ "http://jalammar.github.io/illustrated-transformer/") plus an [annotated paper with code](http://nlp.seas.harvard.edu/annotated-transformer/ "http://nlp.seas.harvard.edu/annotated-transformer/").
- [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556 "https://arxiv.org/abs/2203.15556") by Hoffmann et al. (2022), (the Chinchilla paper).
- [Understanding Large Language Models](https://sebastianraschka.com/blog/2023/llm-reading-list.html "https://sebastianraschka.com/blog/2023/llm-reading-list.html") by Sebastian Raschka.
- [Mamba Explained](https://thegradient.pub/mamba-explained/ "https://thegradient.pub/mamba-explained/") by Kola Ayonrinde.



## Resources
* [Neural Networks: Zero to Hero](https://karpathy.ai/zero-to-hero.html)
* The Complete Mathematics of Neural Networks and Deep Learning: [YouTube](https://www.youtube.com/watch?v=Ixl3nykKG9M)
