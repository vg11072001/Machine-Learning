### Parameter estimation
#### Bias
The bias of an estimator $( \hat{\theta} )$  is the expected difference between ! $\widehat{\theta }$ and the true parameter:
$$[ \text{Bias}(\hat{\theta}) = \mathbb{E}[\hat{\theta}] $$
##### Biased Estimator
A biased estimator is one where the expected value of the estimator is not equal to the true parameter it is estimating. The equation for the bias of an estimator $(\hat{\theta}$ (an estimate of a parameter $(\theta)$ ) is given as: 

$$[ \text{Bias}(\hat{\theta}) = \mathbb{E}[\hat{\theta}] - \theta ] $$

###### Key Components: 
- $(\mathbb{E}[\hat{\theta}])$: 
- The expected value of the estimator. 
- $(\theta)$: The true parameter that we are estimating. 
- 
###### Example: 
If you're estimating the population mean $(\mu)$ using the sample mean $(\hat{\mu})$, and if $(\hat{\mu})$ systematically overestimates or underestimates $(\mu)$, then $(\hat{\mu})$ is biased, and you can calculate the bias using the formula above. 

In practice: 
- If $(\text{Bias}(\hat{\theta}) = 0)$, the estimator is unbiased. 
- If $(\text{Bias}(\hat{\theta}) \neq 0),$ the estimator is biased.

Thus, an estimator is unbiased if its bias is equal to zero, and biased otherwise.

##### Read
[[cheatsheet-statistics_stnfrd_cme_basics.pdf#page=1&selection=16,0,16,20|cheatsheet-statistics_stnfrd_cme_basics, page 1]]
[[cheatsheet-probability_stnfrd_cme_basics.pdf]]

#### Unbiased Estimators

An **unbiased estimator** is a statistical estimator that, on average, produces values equal to the true value of the parameter it is estimating. In other words, an estimator is said to be **unbiased** if the expected value of the estimator equals the parameter being estimated.

##### Key Concepts:
- Let $( \theta )$ be the parameter we want to estimate (e.g., the population mean or variance).
- Let $( \hat{\theta} )$ be the estimator of $\$( \theta )$, calculated from a sample.
- The estimator $( \hat{\theta} )$ is **unbiased** if:
  
  $$ 
  \mathbb{E}[\hat{\theta}] = \theta 
  $$

  where $( \mathbb{E}[\hat{\theta}] )$ represents the expected value (average) of the estimator $( \hat{\theta} )$.

##### Example:
###### 1. Sample Mean:
The sample mean 
$$
( \bar{X} = \frac{1}{n} \sum_{i=1}^{n} X_i )
$$
is an unbiased estimator of the population mean $( \mu )$. This is because:
$$
\mathbb{E}[\bar{X}] = \mu
$$
meaning that, on average, the sample mean equals the population mean.
###### 2. Sample Variance:
The sample variance 
$$ 
( S^2 = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})^2 ) 
$$
is an unbiased estimator of the population variance $( \sigma^2 )$. The factor $$( \frac{1}{n-1} )$$ instead of $( \frac{1}{n} )$ ensures that the sample variance does not underestimate the true population variance.

##### Importance:
- **Unbiasedness** ensures that, on average, we are not systematically overestimating or underestimating the true parameter.
- While an unbiased estimator may not always be the best choice (e.g., it may have high variance), it guarantees that there is no bias in its predictions over many samples.


##### Read :
* [Unbiased Estimator Glossary ](https://www.statlect.com/glossary/unbiased-estimator)
* [What is an unbiased estimator? Proof sample mean is unbiased and why we divide by n-1 for sample var](https://www.youtube.com/watch?v=xJlwSkyeP0k)

### Resources 
* Playlist #Statistics Fundamentals : 60 videos [Link](https://www.youtube.com/playlist?list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9)
* CME 106: #Statistics and #Probability  [Introduction to Probability and Statistics for Engineers](https://stanford.edu/~shervine/teaching/cme-106/)
* Online Stanford Course: #Statistics [Learning with Python](https://www.edx.org/learn/python/stanford-university-statistical-learning-with-python)
- [Why is mathematics important for machine learning engineers and data scientists?](https://uhurasolutions.com/2020/10/14/why-is-mathematics-important-for-machine-learning-engineers-and-data-scientists/)

#### The Math Skills that make Machine Learning easy (and how you can learn it) [Link](https://www.youtube.com/watch?v=wOTFGRSUQ6Q)

- Khan Academy Statistics & Probability [https://www.khanacademy.org/math/stat...](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbFQyNEs5U0JJdXFldnJXeDhpWmU0VlFtbVlNZ3xBQ3Jtc0tuS1pZcUVjTXpZUjlCZWNLSWhEYWJrcTh4OVkyVFFGYWlKT1dRelV4OXQ4VEY5UHRaQnF3OTJZVUlBaEJxZWNoeXpJWDdGaDZfRTlLZENxM0lOQ2pDUGJhZWZZa2o2TXBDWThTYTJVaUZIOGswSjFlQQ&q=https%3A%2F%2Fwww.khanacademy.org%2Fmath%2Fstatistics-probability&v=wOTFGRSUQ6Q) 
- Linear Algebra [https://www.khanacademy.org/math/line...](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbm9wcHA2M1pWaWdUekRFTW4yQ21TYmpJMDRYZ3xBQ3Jtc0tuR0dmUzZmWTNrcFN3dmt1ZDdfQ3BRWW93N0xWMXdUdkRtVjZTT1pTTHZpbnZUcGZtTDlXTE9pQW9Xemp2RXp5dVk3NG13SjUwUDdOTEdqU3Y5dlFXaDNkcmxVaVhoVkM3SnhMRUpVWnM3QmZ6bmloUQ&q=https%3A%2F%2Fwww.khanacademy.org%2Fmath%2Flinear-algebra&v=wOTFGRSUQ6Q) 
- Differential Calculus [https://www.khanacademy.org/math/diff...](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqa0M0bWFhUEduRnEtYzBmVGI0S3lFLV95ZTg0QXxBQ3Jtc0tubzZrQ1VmU1VjeEFVYmFBaUZLb1I2VlZ2Ym1ydHFWTTVCdFNoQmJaVElEd2hJUnRja05Ndi1WV0xMcTFVTW9teXdZbnRUb1dkT0NUd3RManFfc0ZLN1pSX1BmMUhwWEtTQmd3QlJLdGFkMnZwMllvWQ&q=https%3A%2F%2Fwww.khanacademy.org%2Fmath%2Fdifferential-calculus&v=wOTFGRSUQ6Q) 
- An Introduction to Statistical Learning (In R or Python) [Book / PDF ](https://www.statlearning.com/)
- Youtube Playlist:    [![](https://www.gstatic.com/youtube/img/watch/yt_favicon_ringo2.png) • StatsLearning Lecture 1 - part1](https://www.youtube.com/watch?v=5N9V07EIfIg&list=PLOg0ngHtcqbPTlZzRHA2ocQZqB1D_qZ5V&index=1&t=0s)   
- Python Specific Youtube Playlist:    [![](https://www.gstatic.com/youtube/img/watch/yt_favicon_ringo2.png) • Statistical Learning: 1.1 Opening Rem...](https://www.youtube.com/watch?v=LvySJGj-88U&list=PLoROMvodv4rPP6braWoRt5UCXYZ71GZIQ&index=1&t=0s)   
- 3Blue1Brown    [![](https://www.gstatic.com/youtube/img/watch/yt_favicon_ringo2.png) / @3blue1brown](https://www.youtube.com/@3blue1brown)
[Statistical Learning with Python](https://www.youtube.com/playlist?list=PLoROMvodv4rPP6braWoRt5UCXYZ71GZIQ) You tube playlist

![[Pasted image 20241110201605.png]]

![[Pasted image 20241110201249.png]]

![[Pasted image 20241110201951.png]]

![[Pasted image 20241110202015.png]]





#### Extra haven't started:
* Coursera: [Probability & Statistics for Machine Learning & Data Science](https://www.coursera.org/learn/machine-learning-probability-and-statistics)
* [[probability_stats_for_DS.pdf]]
* [[the-elements-of-statistical-learning.pdf]]
* [# Calculus that every Machine Learning Engineer should know👨🏻‍💻👨🏻‍🎓!!](https://pub.aimind.so/calculus-that-every-machine-learning-engineer-should-know-7e44b9a14ad9)
* [# 📈Linear Algebra that every Machine Learning Engineer should know👨🏻‍💻👨🏻‍🎓!!](https://pub.aimind.so/linear-algebra-that-every-data-scientist-should-know-eb585e0ef18d)

#### A Deep Learning Road Map And Where To Start, [Link](https://medium.com/@ArianAmani/the-deep-learning-road-map-that-i-took-c29120b0f5e2) 
> Linear Algebra:

- ==Fast and efficient way:== ==[Coursera Mathematics for Machine Learning: Linear Algebra](https://www.coursera.org/learn/linear-algebra-machine-learning?specialization=mathematics-machine-learning)==
- There’s also this legendary course on Linear Algebra, taught by Prof. Gilbert Strang at MIT, and it’s publicly accessible. Well, I’d really recommend watching this course if you’re really into math and want to learn a whole lot more about linear algebra, and you’ve got the time too. It’s definitely more than enough for starting ML, but if you feel like learning more, go for it: [MIT OCW Linear Algebra 18.06](https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/) [YouTube Playlist](https://www.youtube.com/playlist?list=PL221E2BBF13BECF6C)

> Calculus:

- Fast and efficient way: [Coursera Mathematics for Machine Learning: Multivariate Calculus](https://www.coursera.org/learn/multivariate-calculus-machine-learning?specialization=mathematics-machine-learning)

> Probability and Statistics:

- Fast and efficient way:  
    You can probably learn everything you need at Khan Academy:  
    [https://www.khanacademy.org/math/statistics-probability](https://www.khanacademy.org/math/statistics-probability)
- More deep and academic way:  
    If you would like to dive deeper into the world of probability and statistics, I’d suggest the book **“Probability and Statistics for Engineers and Scientists” by Walpole, Mayers, Ye.**

**These cover the math you need for ML and DL and you won’t need to worry about that part anymore.**

**There’s also this book called** [**“Mathematics for Machine Learning”**](https://mml-book.github.io/) **and it’s free online, I’d suggest reading this instead of all the above if you’re a book person and want something well structured all in one place. But if you think you may let it go in the middle of the book, just stick to the courses above.**

Let’s go to the next part.
#### Mathematics You Need to Work as an ML Engineer in 2024, [Link](https://medium.com/tech-spectrum/minimum-mathematics-you-need-to-work-as-an-ml-engineer-in-the-industry-in-2024-d44cf7de8866)

Becoming a Machine Learning (ML) engineer can be a thrilling and rewarding journey. In today’s industry, ML is at the core of AI, powering everything from recommendation systems to self-driving cars. However, many aspiring ML engineers often wonder, “**How much math do I really need?**” While deep knowledge of mathematics is a powerful asset, the good news is you don’t need a PhD in math to start working in the ML field. What’s essential is knowing the key mathematical concepts that directly impact your work. In this article, I’ll break down the minimum mathematics you need to learn to work as an ML engineer in 2024, and more importantly, where and how these concepts are applied in real-world ML tasks.

#### 1. Linear Algebra

Linear algebra forms the foundation of many ML algorithms and is essential for understanding how data is represented and manipulated. Here’s what you need to focus on:

##### **Key Concepts:**

- **Vectors and Matrices**: The basic data structures in ML. Vectors are used to represent features or weights, and matrices are used to represent entire datasets or transformations.
- **Matrix Multiplication**: Understanding how matrices interact is essential for neural networks and algorithms like principal component analysis (PCA).
- **Eigenvalues and Eigenvectors**: Useful for PCA, which helps in reducing dimensionality and improving the performance of ML models by reducing noise and redundancy.

##### **Where It’s Used:**

- **Deep Learning**: Neural networks operate using matrix multiplication for weight updates.
- **Dimensionality Reduction**: Algorithms like PCA use eigenvectors to find principal components in the data.
- **Transformations**: Rotation, scaling, and projection of data are handled through linear transformations using matrices.

#### 2. Probability and Statistics

ML is all about making predictions from data, and probability helps you measure how likely those predictions are to occur. Statistics allows you to analyze data and draw insights.

##### **Key Concepts:**

- **Probability Distributions**: Understanding distributions (like normal, binomial, and Poisson) is crucial for modeling and making predictions.
- **Bayes’ Theorem**: The backbone of probabilistic modeling, Bayesian inference, and algorithms like Naive Bayes.
- **Statistical Tests**: Hypothesis testing (t-tests, p-values) helps in determining the significance of your findings.
- **Maximum Likelihood Estimation (MLE)**: Used for estimating model parameters to maximize the likelihood of predictions matching reality.

##### **Where It’s Used:**

- **Model Evaluation**: Concepts like precision, recall, F1-score, and ROC curves are rooted in statistics.
- **Bayesian Networks**: In probabilistic graphical models, Bayes’ theorem is used to make inferences from data.
- **A/B Testing**: Common in the industry to compare the performance of two versions of a model or system.

#### 3. Calculus (Especially Derivatives)

Calculus is necessary for understanding how to optimize models, especially in deep learning, where gradient descent is used for training neural networks.

##### **Key Concepts:**

- **Derivatives**: Central to understanding gradients, which measure how changes in the input affect the output.
- **Partial Derivatives**: Key to working with functions that have multiple variables (like loss functions in neural networks).
- **Gradient Descent**: A popular optimization algorithm used to minimize the error of your models by adjusting weights iteratively.

##### **Where It’s Used:**

- **Backpropagation in Neural Networks**: Derivatives are used to update weights through gradient descent.
- **Optimization Problems**: Many ML algorithms like support vector machines (SVM) and logistic regression rely on derivatives to find the best-fit model.
- **Loss Functions**: These measure the difference between predicted and actual values, and minimizing the loss function is the core task in ML model training.

#### 4. Linear Regression and Optimization

Linear regression is often one of the first models ML engineers learn, and optimization is at the heart of making any ML model work well.

##### **Key Concepts:**

- **Ordinary Least Squares (OLS)**: The most common method for linear regression, minimizing the sum of squared errors.
- **Regularization**: Techniques like L1 (Lasso) and L2 (Ridge) regularization are used to prevent overfitting.
- **Convex Optimization**: Understanding convexity ensures that your optimization algorithms (like gradient descent) will converge to a global minimum.

##### **Where It’s Used:**

- **Model Building**: Linear regression helps model relationships between variables and is often the first model tested in predictive tasks.
- **Overfitting Prevention**: Regularization helps avoid overfitting by penalizing large coefficients, ensuring models generalize better to unseen data.

#### 5. Discrete Mathematics

Discrete math is often overlooked, but it’s vital in areas like algorithm analysis, cryptography, and understanding data structures.

##### **Key Concepts:**

- **Combinatorics**: Important in algorithms that deal with permutations and combinations.
- **Graph Theory**: Used in neural networks, decision trees, and recommendation systems (e.g., social network analysis, shortest path algorithms).
- **Boolean Algebra**: Essential for understanding logic gates, decision trees, and binary classification.

##### **Where It’s Used:**

- **Algorithms**: Many algorithms, especially in graph-based problems like social networks or recommendation systems, rely heavily on discrete math.
- **Decision Trees**: Splitting data into binary decisions at each node is based on Boolean algebra.

#### 6. Multivariate Calculus

If you want to work on advanced ML algorithms like deep learning or reinforcement learning, multivariate calculus becomes essential.

##### **Key Concepts:**

- **Multivariable Functions**: Working with functions that take multiple inputs (as most ML models do) requires understanding how changes in one variable affect the overall function.
- **Jacobian and Hessian Matrices**: These are used for optimization problems, helping to understand how sensitive a function is to its input.

##### **Where It’s Used:**

- **Deep Learning**: Training neural networks involves calculating gradients with respect to multiple variables.
- **Optimization**: Understanding the curvature of loss surfaces helps in advanced optimization techniques like second-order methods (e.g., Newton’s method).

#### 7. Advanced Topics (Optional but Helpful)

For specialized areas like NLP (Natural Language Processing), Reinforcement Learning, or advanced Computer Vision, deeper math topics like **Information Theory** or **Tensor Calculus** might come in handy.

While ML requires some foundational mathematics, you don’t need to be overwhelmed. Learning linear algebra, probability, statistics, and calculus will give you a strong base to solve real-world problems. If you’re looking to work in the industry as an ML engineer in 2024, focus on learning these topics and applying them to real projects — your math skills will grow naturally with experience. And remember, ML is more than just math; it’s also about creativity and problem-solving. Good luck on your journey to becoming an ML engineer! If you stay focused on these core mathematical concepts, you’ll be well-prepared for the industry in 2024 and beyond.