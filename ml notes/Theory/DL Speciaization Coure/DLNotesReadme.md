## My notes

- [[C2 DL Improving Deep Neural Networks]]

## DL Available notes:

### 1. Deep Learning Specialization on Coursera  [amanlink](https://aman.ai/coursera-dl/)
(offered by deeplearning.ai)

A distilled compilation of my notes for Coursera's [Deep Learning Specialization (offered by deeplearning.ai)](https://www.coursera.org/specializations/natural-language-processing). The Deep Learning Specialization is a foundational program that will help you understand the capabilities, challenges, and consequences of deep learning and prepare you to participate in the development of leading-edge AI technology. In this Specialization, you will build and train neural network architectures such as Convolutional Neural Networks, Recurrent Neural Networks, LSTMs, Transformers, and learn how to make them better with strategies such as Dropout, BatchNorm, Xavier/He initialization, and more. Get ready to master theoretical concepts and their industry applications using Python and TensorFlow and tackle real-world cases such as speech recognition, music synthesis, chatbots, machine translation, natural language processing, and more.

Notes

#### [Course 1: Neural Networks and Deep Learning](https://aman.ai/coursera-dl/neural-networks-and-deep-learning/)

artificial neural networks; deep learning; backpropagation; python programming

#### [Course 2: Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization](https://aman.ai/coursera-dl/improving-deep-neural-networks/)

mathematical optimization; hyperparameter tuning

#### [Course 3: Structuring Machine Learning Projects](https://aman.ai/coursera-dl/structuring-machine-learning-projects/)

inductive Transfer; machine learning; multi-task learning; decision-making

#### [Course 4: Convolutional Neural Networks](https://aman.ai/coursera-dl/convolutional-neural-networks/)

facial recognition system; convolutional neural network architecture; object detection and segmentation

#### [Course 5: Sequence Models](https://aman.ai/coursera-dl/sequence-models/)

long short term memory (LSTM); gated recurrent unit (GRU); recurrent neural networks; attention models


### 2. CS231n: Convolutional Neural Networks for Visual Recognition [link](https://aman.ai/cs231n/)

A distilled compilation of my notes for Stanford's [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/).  
Stanford's CS231n is one of the best ways to dive into the fields of AI/Deep Learning, and in particular, into Computer Vision. If you plan to excel in another subfield of AI (say, Natural Language Processing or Reinforcement Learning), we still recommend that you start with CS231n, because it helps build intuition, fundamental understanding and hands-on skills.

Notes:

- [Introduction to CNNs for Visual Recognition](https://aman.ai/cs231n/intro/)
- computer vision overview; historical context; course logistics

- [Image Classification](https://aman.ai/cs231n/image-classification/)
- the data-driven approach; k-nearest neighbor; linear classification I

- [Loss Functions](https://aman.ai/cs231n/loss-functions/)
- linear classification II; higher-level representations, image features

- [Optimization](https://aman.ai/cs231n/optimization/)
- optimization, stochastic gradient descent

- [Neural Networks and Backpropagation](https://aman.ai/cs231n/neuralnets-and-backprop/)
- backpropagation; multi-layer perceptron's; the neural viewpoint

- [Convolutional Neural Networks](https://aman.ai/cs231n/cnn/)
- history; convolution and pooling; convnets outside vision

- [Deep Learning Hardware and Software](https://aman.ai/cs231n/deeplearning-HW-SW/)
- CPUs, GPUs, TPUs; PyTorch, TensorFlow; dynamic vs static computation graphs

- [Training Neural Networks I](https://aman.ai/cs231n/training-neural-nets-I/)
- activation functions; data processing; batch normalization; transfer learning

- [Training Neural Networks II](https://aman.ai/cs231n/training-neural-nets-II/)
- update rules; hyperparameter tuning; learning rate scheduling; data augmentation

- [CNN Architectures](https://aman.ai/cs231n/cnn-arch/)
- AlexNet, VGG, GoogLeNet, ResNet, etc.

- [Recurrent Neural Networks](https://aman.ai/cs231n/rnn/)
- RNN, LSTM; language modeling; image captioning; vision + language; attention

- [Generative Models](https://aman.ai/cs231n/generative-models/)
- PixelRNN/PixelCNN; variational auto-encoders; generative adversarial networks

- [Detection and Segmentation](https://aman.ai/cs231n/detection/)
- semantic segmentation; object detection; instance segmentation

- [Visualizing and Understanding](https://aman.ai/cs231n/visualization/)
- feature visualization and inversion; adversarial examples; DeepDream and style transfer


### 3. CS230: Deep Learning

  
A distilled compilation of my notes for Stanford's [CS230: Deep Learning](http://cs230.stanford.edu/).

Notes: 

[Deep Learning Intro and Applications](https://aman.ai/cs230/dl-intro-apps/)

intro; examples of deep learning projects

[Deep Learning Intuition](https://aman.ai/cs230/dl-intuition/)

neural network basics

[Adversarial Examples and GANs](https://aman.ai/cs230/vulnerabilities/)

attacking networks with adversarial/fooling examples; GANs

[Full-cycle of a Deep Learning Project](https://aman.ai/cs230/full-cycle-dl/)

practical aspects of deep learning; optimization

[Deep Learning Strategy](https://aman.ai/cs230/dl-strategy/)

[Interpretability of Neural Networks](https://aman.ai/cs230/interpretability/)

[Deep Reinforcement Learning](https://aman.ai/cs230/deep-rl/)

Markov decision process; Bellman equation; Deep Q-learning

[Chatbots](https://aman.ai/cs230/chatbots/) 