
### CS224N: Natural Language Processing with Deep Learning [Link](https://web.stanford.edu/class/cs224n/)


|Date|Description|Course Materials|Events|Deadlines|
|---|---|---|---|---|
|**Week 1**  <br>  <br>Tue Jan 7|Word Vectors  <br>[[slides](https://web.stanford.edu/class/cs224n/slides_w25/cs224n-2025-lecture01-wordvecs1.pdf)] [[notes](https://web.stanford.edu/class/cs224n/readings/cs224n_winter2023_lecture1_notes_draft.pdf)]|Suggested Readings:<br><br>1. [Efficient Estimation of Word Representations in Vector Space](http://arxiv.org/pdf/1301.3781.pdf) (original word2vec paper)<br>2. [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) (negative sampling paper)|Assignment 1 **out**  <br>[[code](https://web.stanford.edu/class/cs224n/assignments_w25/a1.zip)]||
|Thu Jan 9|Word Vectors and Language Models  <br>[[slides](https://web.stanford.edu/class/cs224n/slides_w25/cs224n-2025-lecture02-wordvecs2.pdf)] [[notes](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes02-wordvecs2.pdf)] [[code](https://web.stanford.edu/class/cs224n/materials/gensim_2024.zip)]|Suggested Readings:<br><br>1. [GloVe: Global Vectors for Word Representation](http://nlp.stanford.edu/pubs/glove.pdf) (original GloVe paper)<br>2. [Improving Distributional Similarity with Lessons Learned from Word Embeddings](http://www.aclweb.org/anthology/Q15-1016)<br>3. [Evaluation methods for unsupervised word embeddings](http://www.aclweb.org/anthology/D15-1036)<br><br>Additional Readings:<br><br>1. [A Latent Variable Model Approach to PMI-based Word Embeddings](http://aclweb.org/anthology/Q16-1028)<br>2. [Linear Algebraic Structure of Word Senses, with Applications to Polysemy](https://transacl.org/ojs/index.php/tacl/article/viewFile/1346/320)<br>3. [On the Dimensionality of Word Embedding](https://papers.nips.cc/paper/7368-on-the-dimensionality-of-word-embedding.pdf)|||
|Fri Jan 10|Python Review Session  <br>[[slides](https://web.stanford.edu/class/cs224n/slides_w25/2024%20CS224N%20Python%20Review%20Session%20Slides.pptx.pdf)] [[colab](https://colab.research.google.com/drive/1hxWtr98jXqRDs_rZLZcEmX_hUcpDLq6e?usp=sharing)]|Time 1:30pm-2:20pm  <br>Location Gates B01|||
|**Week 2**  <br>  <br>Tue Jan 14|Backpropagation and Neural Network Basics  <br>[[slides](https://web.stanford.edu/class/cs224n/slides_w25/cs224n-2025-lecture03-neuralnets.pdf)] [[notes](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes03-neuralnets.pdf)]|Suggested Readings:<br><br>1. [matrix calculus notes](https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf)<br>2. [Review of differential calculus](https://web.stanford.edu/class/cs224n/readings/review-differential-calculus.pdf)<br>3. [CS231n notes on network architectures](http://cs231n.github.io/neural-networks-1/)<br>4. [CS231n notes on backprop](http://cs231n.github.io/optimization-2/)<br>5. [Derivatives, Backpropagation, and Vectorization](http://cs231n.stanford.edu/handouts/derivatives.pdf)<br>6. [Learning Representations by Backpropagating Errors](http://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf) (seminal Rumelhart et al. backpropagation paper)<br><br>Additional Readings:<br><br>1. [Yes you should understand backprop](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b)<br>2. [Natural Language Processing (Almost) from Scratch](http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf)|Assignment 2 **out**  <br>[[code](https://web.stanford.edu/class/cs224n/assignments_w25/a2.zip)]  <br>[[handout](https://web.stanford.edu/class/cs224n/assignments_w25/a2.pdf)]  <br>[[latex template](https://web.stanford.edu/class/cs224n/assignments_w25/a2_tex.zip)]|Assignment 1 **due**|
|Thu Jan 16|Dependency Parsing  <br>[[slides](https://web.stanford.edu/class/cs224n/slides_w25/cs224n-2025-lecture04-dep-parsing.pdf)] [[notes](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes04-dependencyparsing.pdf)]|Suggested Readings:<br><br>1. [Incrementality in Deterministic Dependency Parsing](https://www.aclweb.org/anthology/W/W04/W04-0308.pdf)<br>2. [A Fast and Accurate Dependency Parser using Neural Networks](https://www.emnlp2014.org/papers/pdf/EMNLP2014082.pdf)<br>3. [Dependency Parsing](https://link.springer.com/book/10.1007/978-3-031-02131-2)<br>4. [Globally Normalized Transition-Based Neural Networks](https://arxiv.org/pdf/1603.06042.pdf)<br>5. [Universal Stanford Dependencies: A cross-linguistic typology](http://nlp.stanford.edu/~manning/papers/USD_LREC14_UD_revision.pdf)<br>[](http://nlp.stanford.edu/~manning/papers/USD_LREC14_UD_revision.pdf)7. [](http://nlp.stanford.edu/~manning/papers/USD_LREC14_UD_revision.pdf)[Universal Dependencies website](http://universaldependencies.org/)<br>8. [Jurafsky & Martin Chapter 19](https://web.stanford.edu/~jurafsky/slp3/19.pdf)|||
|Fri Jan 17|PyTorch Tutorial Session  <br>[[colab](https://colab.research.google.com/drive/1Pz8b_h-W9zIBk1p2e6v-YFYThG1NkYeS?usp=sharing)]|Time 1:30pm-2:20pm  <br>Location Gates B01|||
|**Week 3**  <br>  <br>Tue Jan 21|Basic Sequence Models to RNNs  <br>[[slides](https://web.stanford.edu/class/cs224n/slides_w25/cs224n-2025-lecture05-rnnlm.pdf)] [[notes (lectures 5 and 6)](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes05-LM_RNN.pdf)]|Suggested Readings:<br><br>1. [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf) (textbook chapter)<br>2. [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) (blog post overview)<br>3. [Sequence Modeling: Recurrent and Recursive Neural Nets](http://www.deeplearningbook.org/contents/rnn.html) (Sections 10.1 and 10.2)<br>4. [On Chomsky and the Two Cultures of Statistical Learning](http://norvig.com/chomsky.html)|
|Thu Jan 23|Advanced Variants of RNNs, Attention  <br>[[slides](https://web.stanford.edu/class/cs224n/slides_w25/cs224n-2025-lecture06-fancy-rnn.pdf)] [[notes (lectures 5 and 6)](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes05-LM_RNN.pdf)]|Suggested Readings:<br><br>1. [Learning long-term dependencies with gradient descent is difficult](https://ieeexplore.ieee.org/document/279181) (one of the original vanishing gradient papers)<br>2. [On the difficulty of training Recurrent Neural Networks](https://arxiv.org/pdf/1211.5063.pdf) (proof of vanishing gradient problem)<br>3. [Vanishing Gradients Jupyter Notebook](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/lectures/vanishing_grad_example.html) (demo for feedforward networks)<br>4. [Attention Is All You Need](https://arxiv.org/abs/1706.03762.pdf)|Assignment 3 **out**  <br>[[code](https://web.stanford.edu/class/cs224n/assignments_w25/a3.zip)]  <br>[[handout](https://web.stanford.edu/class/cs224n/assignments_w25/a3.pdf)]  <br>[[latex template](https://web.stanford.edu/class/cs224n/assignments_w25/a3_tex.zip)]|Assignment 2 **due**|
|**Week 4**  <br>  <br>Tue Jan 28|Final Projects: Custom and Default; Practical Tips  <br>[[slides](https://web.stanford.edu/class/cs224n/slides_w25/cs224n-2025-lecture07-final-project.pdf)]|Suggested Readings:<br><br>1. [Practical Methodology](https://www.deeplearningbook.org/contents/guidelines.html) (_Deep Learning_ book chapter)|||
|Thu Jan 30|Transformers [[slides](https://web.stanford.edu/class/cs224n/slides_w25/cs224n-2025-lecture08-transformers.pdf)] [[Custom project tips](https://web.stanford.edu/class/cs224n/project/custom-final-project-tips.pdf)] [[notes](https://web.stanford.edu/class/cs224n/readings/cs224n-self-attention-transformers-2023_draft.pdf)]|Suggested Readings:<br><br>1. [Attention Is All You Need](https://arxiv.org/abs/1706.03762.pdf)<br>2. [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)<br>3. [Transformer (Google AI blog post)](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)<br>4. [Layer Normalization](https://arxiv.org/pdf/1607.06450.pdf)<br>5. [Image Transformer](https://arxiv.org/pdf/1802.05751.pdf)<br>6. [Music Transformer: Generating music with long-term structure](https://arxiv.org/pdf/1809.04281.pdf)<br>7. [Jurafsky and Martin Chapter 9 (The Transformer)](https://web.stanford.edu/~jurafsky/slp3/9.pdf)|Project Proposal **out**  <br>[[handout](https://web.stanford.edu/class/cs224n/project/project-proposal-instructions-spr2024-updated.pdf)]  <br>  <br>Default Final Project **out**  <br>[[handout](https://web.stanford.edu/class/cs224n/project_w25/CS_224n__Default_Final_Project__Build_GPT_2.pdf)]||
|**Week 5**  <br>  <br>Tue Feb 4|Pretraining  <br>[[slides](https://web.stanford.edu/class/cs224n/slides_w25/cs224n-2025-lecture09-pretraining.pdf)]|Suggested Readings:<br><br>1. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)<br>2. [Contextual Word Representations: A Contextual Introduction](https://arxiv.org/abs/1902.06006.pdf)<br>3. [The Illustrated BERT, ELMo, and co.](http://jalammar.github.io/illustrated-bert/)<br>4. [Jurafsky and Martin Chapter 11 (Masked Language Models)](https://web.stanford.edu/~jurafsky/slp3/11.pdf)|Assignment 4 **out**  <br>[[code](https://web.stanford.edu/class/cs224n/assignments_w25/a4.zip)]  <br>[[handout](https://web.stanford.edu/class/cs224n/assignments_w25/a4.pdf)]  <br>[[overleaf](https://web.stanford.edu/class/cs224n/assignments_w25/a4_tex.zip)]  <br>[[colab run script](https://colab.research.google.com/drive/1vp_6RYYqMhjVIzt2MRhuWnBBmz4BJ30j?usp=sharing)]|Assignment 3 **due**|
|Thu Feb 6|Post-training (RLHF, SFT, DPO) [[slides](https://web.stanford.edu/class/cs224n/slides_w25/cs224n-2025-lecture10-instruction-tunining-rlhf.pdf)]|Suggested Readings:<br><br>1. [Aligning language models to follow instructions](https://openai.com/research/instruction-following)<br>2. [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416)<br>3. [AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback](https://arxiv.org/abs/2305.14387)<br>4. [How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources](https://arxiv.org/abs/2306.04751)<br>5. [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290)|||
|Fri Feb 7|Hugging Face Transformers Tutorial Session  <br>[[colab](https://colab.research.google.com/drive/13r94i6Fh4oYf-eJRSi7S_y_cen5NYkBm#scrollTo=OTsW-Wwi-X81)]|Time 1:30pm-2:20pm  <br>Location Gates B01|||
|**Week 6**  <br>  <br>Tue Feb 11|Efficient Adaptation (Prompting + PEFT)|Suggested Readings:<br><br>1. [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)<br>2. [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)<br>3. [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635)<br>4. [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)<br>5. [Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/abs/1902.00751)||Project Proposal **due**|
|Thu Feb 13|Benchmarking and Evaluation|Suggested Readings:<br><br>1. [Challenges and Opportunities in NLP Benchmarking](https://www.ruder.io/nlp-benchmarking/)<br>2. [Measuring Massive Multitask Language Understanding](https://arxiv.org/abs/2009.03300)<br>3. [Holistic Evaluation of Language Models](https://arxiv.org/abs/2211.09110)<br>4. [AlpacaEval](https://tatsu-lab.github.io/alpaca_eval/)|Project Milestone **out**|Assignment 4 **due**|
|**Week 7**  <br>  <br>Tue Feb 18|Question Answering and Knowledge|Suggested readings:<br><br>1. [SQuAD: 100,000+ Questions for Machine Comprehension of Text](https://arxiv.org/abs/1606.05250)<br>2. [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906)<br>3. [Bidirectional Attention Flow for Machine Comprehension](https://arxiv.org/abs/1611.01603)<br>4. [Reading Wikipedia to Answer Open-Domain Questions](https://arxiv.org/abs/1704.00051)<br>5. [REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909)<br>6. [Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/abs/2307.03172)||Final Project Proposals **Returned**|
|Thu Feb 20|Guest Lecture ([Alexander Rush](https://rush-nlp.com/))|Suggested readings:|||
|**Week 8**  <br>  <br>Tue Feb 25|Guest Lecture ([Sewon Min](https://www.sewonmin.com/))|Suggested readings:||Final Project Milestone **due**|
|Thu Feb 27|Guest Lecture ([Shunyu Yao](https://ysymyth.github.io/))|Suggested readings:|Final Project Report Instructions **out**||
|Fri Feb 28||||Course Withdrawal **Deadline**|
|**Week 9**  <br>  <br>Tue Mar 4|Guest Lecture ([Jing Huang](https://explanare.github.io/))|Suggested readings:|Final Project Milestones **Returned**||
|Thu Mar 6|Guest Lecture ([Noam Brown](https://noambrown.github.io/))|Suggested readings:|||
|**Week 10**  <br>  <br>Tue Mar 11|Open Questions in NLP 2025||||
|Thu Mar 13|Final Project Emergency Assistance (No Lecture)|||Final project **due**|
|Finals Week|Final Project Poster Session|Time: Mar 18th, Time 12:15pm-3:15pm  <br>Location TBD  <br>On-campus students must attend in person!||[[Printing guide](https://docs.google.com/document/d/1J8-TVVvndimSwq3jGzpMO_OtPAx_EaU9nUiiPUQdVpY)]|
