{"path":"cheatsheet-statistics_stnfrd_cme_basics.pdf","text":"CME 106 – Introduction to Probability and Statistics for Engineers https://stanford.edu/~shervine VIP Cheatsheet: Statistics Afshine Amidi and Shervine Amidi September 8, 2020 Paramater estimation r Random sample – A random sample is a collection of n random variables X1, ..., Xn that are independent and identically distributed with X. r Estimator – An estimator ˆθ is a function of the data that is used to infer the value of an unknown parameter θ in a statistical model. r Bias – The bias of an estimator ˆθ is deﬁned as being the diﬀerence between the expected value of the distribution of ˆθ and the true value, i.e.: Bias(ˆθ) = E[ˆθ] − θ Remark: an estimator is said to be unbiased when we have E[ˆθ] = θ. r Sample mean and variance – The sample mean and the sample variance of a random sample are used to estimate the true mean µ and the true variance σ2 of a distribution, are noted X and s2 respectively, and are such that: X = 1 n n∑ i=1 Xi and s2 = ˆσ2 = 1 n − 1 n∑ i=1 (Xi − X) 2 r Central Limit Theorem – Let us have a random sample X1, ..., Xn following a given distribution with mean µ and variance σ2, then we have: X ∼ n→+∞ N (µ, σ √n ) Conﬁdence intervals r Conﬁdence level – A conﬁdence interval CI1−α with conﬁdence level 1 − α of a true pa- rameter θ is such that 1 − α of the time, the true value is contained in the conﬁdence interval: P (θ ∈ CI1−α) = 1 − α r Conﬁdence interval for the mean – When determining a conﬁdence interval for the mean µ, diﬀerent test statistics have to be computed depending on which case we are in. The following table sums it up: Distribution Sample size σ2 Statistic 1 − α conﬁdence interval any known X − µ σ√ n ∼ N (0,1) [X − z α 2 σ√ n ,X + z α 2 σ√n ] Xi ∼ N (µ, σ) small unknown X − µ s√n ∼ tn−1 [X − t α 2 s√ n ,X + t α 2 s√n ] known X − µ σ√ n ∼ N (0,1) [X − z α 2 σ√ n ,X + z α 2 σ√n ] Xi ∼ any large unknown X − µ s√ n ∼ N (0,1) [X − z α 2 s√ n ,X + z α 2 s√n ] Xi ∼ any small any Go home! Go home! r Conﬁdence interval for the variance – The single-line table below sums up the test statistic to compute when determining the conﬁdence interval for the variance. Distribution Sample size µ Statistic 1 − α conﬁdence interval Xi ∼ N (µ,σ) any any s2(n − 1) σ2 ∼ χ2 n−1 [ s2(n−1) χ2 2 , s 2(n−1) χ2 1 ] Hypothesis testing r Errors – In a hypothesis test, we note α and β the type I and type II errors respectively. By noting T the test statistic and R the rejection region, we have: α = P (T ∈ R|H0 true) and β = P (T /∈ R|H1 true) r p-value – In a hypothesis test, the p-value is the probability under the null hypothesis of having a test statistic T at least as extreme as the one that we observed T0. We have: Case Left-sided Right-sided Two-sided p-value P (T ⩽ T0|H0 true) P (T ⩾ T0|H0 true) P (|T | ⩾ |T0||H0 true) r Sign test – The sign test is a non-parametric test used to determine whether the median of a sample is equal to the hypothesized median. By noting V the number of samples falling to the right of the hypothesized median, we have: Statistic when np < 5 Statistic when np ⩾ 5 V ∼ H0 B (n, p = 1 2 ) Z = V − n 2 √n 2 ∼ H0 N (0,1) r Testing for the diﬀerence in two means – The table below sums up the test statistic to compute when performing a hypothesis test where the null hypothesis is: H0 : µX − µY = δ Stanford University 1 Winter 2018 CME 106 – Introduction to Probability and Statistics for Engineers https://stanford.edu/~shervine Distribution of Xi, Yi nX , nY σ2 X , σ2 Y Statistic any known (X − Y ) − δ √ σ2 X nX + σ2 Y nY ∼ H0 N (0,1) Normal large unknown (X − Y ) − δ √ s2 X nX + s2 Y nY ∼ H0 N (0,1) small unknown σX = σY (X − Y ) − δ s√ 1 nX + 1 nY ∼ H0 tnX +nY −2 Normal, paired any unknown D − δ sD√n ∼ H0 tn−1 Di = Xi − Yi nX = nY r χ2 goodness of ﬁt test – By noting k the number of bins, n the total number of samples, pi the probability of success in each bin and Yi the associated number of samples, we can use the test statistic T deﬁned below to test whether or not there is a good ﬁt. If npi ⩾ 5, we have: T = k∑ i=1 (Yi − npi)2 npi ∼ H0 χ2 df with df = (k − 1) − #(estimated parameters) r Test for arbitrary trends – Given a sequence, the test for arbitrary trends is a non- parametric test, whose aim is to determine whether the data suggest the presence of an increasing trend: H0 : no trend versus H1 : there is an increasing trend If we note x the number of transpositions in the sequence, the p-value is computed as: p-value = P (T ⩽ x) Regression analysis In the following section, we will note (x1, Y1), ..., (xn, Yn) a collection of n data points. r Simple linear model – Let X be a deterministic variable and Y a dependent random variable. In the context of a simple linear model, we assume that Y is linked to X via the regression coeﬃcients α, β and a random variable e ∼ N (0,σ), where e is referred as the error. We estimate Y, α, β by ˆY , A, B and have: Y = α + βX + e and ˆYi = A + Bxi r Notations – Given n data points (xi, Yi), we deﬁne SXY ,SXX and SY Y as follows: SXY = n∑ i=1 (xi − x)(Yi − Y ) and SXX = n∑ i=1 (xi − x)2 and SY Y = n∑ i=1 (Yi − Y )2 r Sum of squared errors – By keeping the same notations, we deﬁne the sum of squared errors, also known as SSE, as follows: SSE = n∑ i=1 (Yi − ˆYi) 2 = n∑ i=1 (Yi − (A + Bxi))2 = SY Y − BSXY r Least-squares estimates – When estimating the coeﬃcients α, β with the least-squares method which is done by minimizing the SSE, we obtain the estimates A, B deﬁned as follows: A = Y − SXY SXX x and B = SXY SXX r Key results – When σ is unknown, this parameter is estimated by the unbiased estimator s2 deﬁned as follows: s2 = SY Y − BSXY n − 2 and we have s2(n − 2) σ2 ∼ χ2 n−2 The table below sums up the properties surrounding the least-squares estimates A, B when σ is known or not: Coeﬀ σ Statistic 1 − α conﬁdence interval known A − α σ√ 1 n + X2 SXX ∼ N (0,1) [ A − z α 2 σ√ 1 n + X2 SXX ,A + z α 2 σ√ 1 n + X2 SXX ] α unknown A−α s√ 1 n + X2 SXX ∼ tn−2 [ A − t α 2 s√ 1 n + X2 SXX ,A + t α 2 s√ 1 n + X2 SXX ] known B−β σ√SXX ∼ N (0,1) [ B − z α 2 σ√SXX ,B + z α 2 σ√SXX ] β unknown B−β s√SXX ∼ tn−2 [ B − t α 2 s√SXX ,B + t α 2 s√SXX ] Correlation analysis r Sample correlation coeﬃcient – The correlation coeﬃcient is in practice estimated by the sample correlation coeﬃcient, often noted r or ˆρ, which is deﬁned as: r = ˆρ = SXY √SXX SY Y with r√n − 2 √1 − r2 ∼ H0 tn−2 for H0 : ρ = 0 r Correlation properties – By noting V1 = V − z α 2√n−3 , V2 = V + z α 2√ n−3 with V = 1 2 ln ( 1+r 1−r ), the table below sums up the key results surrounding the correlation coeﬃcient estimate: Sample size Standardized statistic 1 − α conﬁdence interval for ρ large V − 1 2 ln ( 1+ρ 1−ρ ) 1√ n−3 ∼ n≫1 N (0,1) [ e2V1 − 1 e2V1 + 1 , e2V2 − 1 e2V2 + 1 ] Stanford University 2 Winter 2018","libVersion":"0.3.2","langs":""}