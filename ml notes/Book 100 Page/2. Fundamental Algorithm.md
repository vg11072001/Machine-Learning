## 1. Linear Regression

## 2. Logistic Regression

## 3. Decision Tree Learning

#### C4.5 and ID3 comparison
- [[2.3 DecisionTreeExtended.pdf]]
	*  C4.5 is one of the most widely used decision tree learning algorithms. It can be seen as an improved version of ID3. It have
	* Information gain:
		* instead of trying to minimize the entropy, we look for a split that maximizes the information gain.
		* , Information Gain measures how much "information" a feature provides by reducing the uncertainty in predicting the target variable after splitting the dataset based on that feature. A higher Information Gain indicates a more informative feature, and this helps in making decisions at each node of a decision tree.
		* Information Gain (IG) measures how much a given feature helps in reducing the uncertainty (entropy) in a dataset. It is used in decision trees to determine the best feature for splitting the data. The formula for Information Gain is: 
		* $$ G(S, j) = H(S) − \sum_{k \in V(S, j)} \frac{|S_{j,k}|}{|S|} H(S_{j,k})$$ 
			* where: 
				- $( H(S) )$ is the entropy of the dataset $( S )$ before the split, 
				- $( V(S, j) )$ represents the set of possible values that the feature \( j \) can take, 
				- $( S_{j,k} )$ represents the subset of \( S \) where the feature \( j \) takes the value \( k \), 
				- $( |S_{j,k}| )$ is the number of instances in \( S \) where the feature \( j \) takes the value \( k \), 
				- $( |S| )$ is the total number of instances in the dataset. 
		* Explanation: - 
			* $( H(S) )$ gives the original entropy of the dataset \( S \) before applying the split based on feature \( j \). 
			- After splitting \( S \) based on the possible values \( k \) of feature \( j \), the entropy of each subset $( S_{j,k} )$ is calculated, and the weighted sum of these entropies is taken. 
			- The **information gain** is the reduction in entropy (i.e., the difference between the original entropy and the weighted sum of the entropies after splitting).
	* C4.5 can deal with **missing values**
	* The C4.5 algorithm stops in some leaf node (decides not to split it) in one of the following cases: 
		* All the examples in the leaf node belong to the same class. 
		* None of the features provide any information gain.
	* **Overfitting prevention** mechanism: Once the tree is built, C4.5 replaces some branches (also called subtrees) by leaf nodes. Doing that reduces variance (but inevitably increases bias as we remember from the bias-variance tradeoff principle)
	* We keep **pruning** the tree as long as pruning helps to reduce the error on the validation data.


- [A useful view of decision trees](https://www.benkuhn.net/tree-imp "https://www.benkuhn.net/tree-imp") by Ben Kuhn

## 4. Support Vector Machine

## 5. K-Nearest neighbors

