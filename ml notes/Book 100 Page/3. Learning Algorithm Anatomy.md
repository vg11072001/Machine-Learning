
You may have noticed by reading the previous chapter that each learning algorithm we saw consisted of three parts:
1) a loss function;
2) an optimization criterion based on the loss function (a cost function, for example); 
3) an optimization routine leveraging training data to find a solution to the optimization
criterion.

#### Gradient Descent



#### Stochastic Gradient Descent


### Optimizer for DL reading:

###### Medium [Understanding Deep Learning Optimizers: Momentum, AdaGrad, RMSProp & Adam](https://towardsdatascience.com/understanding-deep-learning-optimizers-momentum-adagrad-rmsprop-adam-e311e377e9c2)

###### TowardsDS [A Visual Explanation of Gradient Descent Methods (Momentum, AdaGrad, RMSProp, Adam)](https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c)

